{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print(text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thr-ee', 'hund-------///red', 'and', 'forty-eight', 'years,', 'six', 'months,', 'and', 'nineteen', 'days', 'ago\\nto-day,', 'the', 'Parisians', 'awoke', 'to', 'the', 'sound', 'of', 'all', 'the', 'bells', 'in', 'the', 'triple\\ncircuit', 'of', 'the', 'city,', 'the', 'university,', 'and', 'the', 'town', 'ringing', 'a', 'full', 'peal.\\n\\nThe', 'sixth', 'of', 'January,', '1482,', 'is', 'not,', 'however,', 'a', 'day', 'of', 'which', 'history', 'has\\npreserved', 'the', 'memory.', 'There', 'was', 'nothing', 'notable', 'in', 'the', 'event', 'which\\nthus', 'set', 'the', 'bells', 'and', 'the', 'bourgeois', 'of', 'Paris', 'in', 'a', 'ferment', 'from', 'early\\nmorning.', 'It', 'was', 'neither', 'an', 'assault', 'by', 'the', 'Picards', 'nor', 'the', 'Burgundians,\\nnor', 'a', 'hunt', 'led', 'along', 'in', 'procession,', 'nor', 'a', 'revolt', 'of', 'scholars', 'in', 'the', 'town\\nof', 'Laas,', 'nor', 'an']\n"
     ]
    }
   ],
   "source": [
    "# we split first 100 words with delimiter WHITESPACE default - > it will work removing (sep= \" \")\n",
    "# we can define any value in seperator like \". , ;\" etc\n",
    "words = text.split(sep=\" \")\n",
    "print(words[0:100]) # can be written as [:100] also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thr', 'ee', 'hund', 'red', 'and', 'forty', 'eight', 'years', 'six', 'months', 'and', 'nineteen', 'days', 'ago', 'to', 'day', 'the', 'Parisians', 'awoke', 'to', 'the', 'sound', 'of', 'all', 'the', 'bells', 'in', 'the', 'triple', 'circuit', 'of', 'the', 'city', 'the', 'university', 'and', 'the', 'town', 'ringing', 'a', 'full', 'peal', 'The', 'sixth', 'of', 'January', '1482', 'is', 'not', 'however', 'a', 'day', 'of', 'which', 'history', 'has', 'preserved', 'the', 'memory', 'There', 'was', 'nothing', 'notable', 'in', 'the', 'event', 'which', 'thus', 'set', 'the', 'bells', 'and', 'the', 'bourgeois', 'of', 'Paris', 'in', 'a', 'ferment', 'from', 'early', 'morning', 'It', 'was', 'neither', 'an', 'assault', 'by', 'the', 'Picards', 'nor', 'the', 'Burgundians', 'nor', 'a', 'hunt', 'led', 'along', 'in', 'procession']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "filename = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])\n",
    "#print(text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "pun = (string.punctuation)\n",
    "print(pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thr-ee', 'hund-------///red', 'and', 'forty-eight', 'years,', 'six', 'months,', 'and', 'nineteen', 'days', 'ago\\nto-day,', 'the', 'parisians', 'awoke', 'to', 'the', 'sound', 'of', 'all', 'the', 'bells', 'in', 'the', 'triple\\ncircuit', 'of', 'the', 'city,', 'the', 'university,', 'and', 'the', 'town', 'ringing', 'a', 'full', 'peal.\\n\\nthe', 'sixth', 'of', 'january,', '1482,', 'is', 'not,', 'however,', 'a', 'day', 'of', 'which', 'history', 'has\\npreserved', 'the', 'memory.', 'there', 'was', 'nothing', 'notable', 'in', 'the', 'event', 'which\\nthus', 'set', 'the', 'bells', 'and', 'the', 'bourgeois', 'of', 'paris', 'in', 'a', 'ferment', 'from', 'early\\nmorning.', 'it', 'was', 'neither', 'an', 'assault', 'by', 'the', 'picards', 'nor', 'the', 'burgundians,\\nnor', 'a', 'hunt', 'led', 'along', 'in', 'procession,', 'nor', 'a', 'revolt', 'of', 'scholars', 'in', 'the', 'town\\nof', 'laas,', 'nor', 'an']\n"
     ]
    }
   ],
   "source": [
    "filename = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print(text)\n",
    "file.close()\n",
    "\n",
    "# we split first 100 words with delimiter WHITESPACE default - > it will work removing (sep= \" \")\n",
    "# we can define any value in seperator like \". , ;\" etc\n",
    "\n",
    "words = text.split(sep=\" \")\n",
    "pun = (string.punctuation)\n",
    "st='\\n'\n",
    "# remove punctuation from each word\n",
    "# use function called \"translate()\" that will map one set of characters to another\n",
    "#use the function \"maketrans()\" to create a mapping table. We can create an empty mapping table, \n",
    "#but the third argument of this function allows us to list all of the characters to remove during the translation process\n",
    "\n",
    "\n",
    "#lowercase = [w.lower() for w in words]\n",
    "#print(lowercase[:100]) #print all words in lower case\n",
    "\n",
    "table = str.maketrans('','',pun)\n",
    "\n",
    "stripped = [w.translate(table) for w in words]\n",
    "#print(stripped[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on crossing the threshold of that grand hall, in the midst of that\\n']\n",
      "on crossing the threshold of that grand hall, in the midst of that\n",
      "\n",
      "on crossing the threshold of that grand hall, in the midst of that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "file_name = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(file_name, \"r\")\n",
    "filedata = file.readlines()\n",
    "article = filedata[100].split(\". \")\n",
    "print(article)\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = 0\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    \n",
    "    similarity_matrix = np.zeros(len(sentences), len(sentences))\n",
    "    \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "            \n",
    "return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thr-ee hund-------///red and forty-eight years, six months, and nineteen days ago\n",
      "to-day, the Parisians awoke to the sound of all the bells in the triple\n",
      "circuit of the city, the university, and the town ringing a full peal.\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenizer - > sent_tokenize() function to split text into sentences.\n",
    "#From below file into memory, splits it into sentences, and prints the first sentence.\n",
    "\n",
    "filename = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(filename,'rt')\n",
    "text = file.read()\n",
    "#print(text)\n",
    "#words = text.split(\" \")\n",
    "#print(words[0:100])\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thr-ee', 'hund', '--', '--', '--', '-///red', 'and', 'forty-eight', 'years', ',', 'six', 'months', ',', 'and', 'nineteen', 'days', 'ago', 'to-day', ',', 'the', 'Parisians', 'awoke', 'to', 'the', 'sound', 'of', 'all', 'the', 'bells', 'in', 'the', 'triple', 'circuit', 'of', 'the', 'city', ',', 'the', 'university', ',', 'and', 'the', 'town', 'ringing', 'a', 'full', 'peal', '.', 'The', 'sixth', 'of', 'January', ',', '1482', ',', 'is', 'not', ',', 'however', ',', 'a', 'day', 'of', 'which', 'history', 'has', 'preserved', 'the', 'memory', '.', 'There', 'was', 'nothing', 'notable', 'in', 'the', 'event', 'which', 'thus', 'set', 'the', 'bells', 'and', 'the', 'bourgeois', 'of', 'Paris', 'in', 'a', 'ferment', 'from', 'early', 'morning', '.', 'It', 'was', 'neither', 'an', 'assault', 'by']\n"
     ]
    }
   ],
   "source": [
    "# word tokenizer - > word_tokenize() function used for splitting strings into tokens (nominally words).\n",
    "#It splits tokens based on white space and punctuation\n",
    "#From below file into memory, splits it into sentences, and prints the first sentence.\n",
    "\n",
    "filename = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(filename,'rt')\n",
    "text = file.read()\n",
    "#print(text)\n",
    "#words = text.split(\" \")\n",
    "#print(words[0:100])\n",
    "word_t = word_tokenize(text)\n",
    "print(word_t[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hund', 'and', 'years', 'six', 'months', 'and', 'nineteen', 'days', 'ago', 'the', 'Parisians', 'awoke', 'to', 'the', 'sound', 'of', 'all', 'the', 'bells', 'in', 'the', 'triple', 'circuit', 'of', 'the', 'city', 'the', 'university', 'and', 'the', 'town', 'ringing', 'a', 'full', 'peal', 'The', 'sixth', 'of', 'January', 'is', 'not', 'however', 'a', 'day', 'of', 'which', 'history', 'has', 'preserved', 'the', 'memory', 'There', 'was', 'nothing', 'notable', 'in', 'the', 'event', 'which', 'thus', 'set', 'the', 'bells', 'and', 'the', 'bourgeois', 'of', 'Paris', 'in', 'a', 'ferment', 'from', 'early', 'morning', 'It', 'was', 'neither', 'an', 'assault', 'by', 'the', 'Picards', 'nor', 'the', 'Burgundians', 'nor', 'a', 'hunt', 'led', 'along', 'in', 'procession', 'nor', 'a', 'revolt', 'of', 'scholars', 'in', 'the', 'town']\n"
     ]
    }
   ],
   "source": [
    "# use of isalpha() - > Return true if all characters in the string are alphabetic and at least one character, false otherwise.\n",
    "#Alphabetic characters are those characters defined in the Unicode character database.\n",
    "\n",
    "word_t = word_tokenize(text)\n",
    "clean_words = [w for w in word_t if w.isalpha()]\n",
    "print(clean_words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# stopwords - > stopwords are those words that do not contribute to the deeper meaning of the phrase.\n",
    "#They are the most common words such as: “the“, “a“, and “is“.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_w = stopwords.words('english')\n",
    "print(stop_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thr-ee', 'hund', '--', '--', '--', '-///red', 'and', 'forty-eight', 'year', ',', 'six', 'month', ',', 'and', 'nineteen', 'day', 'ago', 'to-day', ',', 'the', 'parisian', 'awok', 'to', 'the', 'sound', 'of', 'all', 'the', 'bell', 'in', 'the', 'tripl', 'circuit', 'of', 'the', 'citi', ',', 'the', 'univers', ',', 'and', 'the', 'town', 'ring', 'a', 'full', 'peal', '.', 'the', 'sixth', 'of', 'januari', ',', '1482', ',', 'is', 'not', ',', 'howev', ',', 'a', 'day', 'of', 'which', 'histori', 'ha', 'preserv', 'the', 'memori', '.', 'there', 'wa', 'noth', 'notabl', 'in', 'the', 'event', 'which', 'thu', 'set', 'the', 'bell', 'and', 'the', 'bourgeoi', 'of', 'pari', 'in', 'a', 'ferment', 'from', 'earli', 'morn', '.', 'It', 'wa', 'neither', 'an', 'assault', 'by']\n"
     ]
    }
   ],
   "source": [
    "# stemming of words \n",
    "\n",
    "#PorterStemmer  - https://tartarus.org/martin/PorterStemmer/\n",
    "\n",
    "#Stemming refers to the process of reducing each word to its root or base.\n",
    "#For example “fishing,” “fished,” “fisher” all reduce to the stem “fish.”\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "filename = 'H:/Python In Data Science/Deep Learning for NLP/Exercise/Notre Dame de Paris.txt'\n",
    "file = open(filename,'rt')\n",
    "text = file.read()\n",
    "#print(text)\n",
    "#words = text.split(\" \")\n",
    "#print(words[0:100])\n",
    "word_t = word_tokenize(text)\n",
    "#word_t = [w.lower() for w in word_t]\n",
    "\n",
    "prt = PorterStemmer()\n",
    "stm = [prt.stem(w) for w in word_t]\n",
    "print(stm[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
